\chapter{Benchmark}

The main aim of this thesis is to methodically examine the appearance phenomena that are frequently appearing in our day-to-day life but for some reason are still rarely implemented in the modern renderers. However, in this past few years, the need for the physically realistic renders has grown significantly and the implementations for these phenomena have been introduced. As they are still being consistently improved and integrated into modern mainstream renderers, it is absolutely necessary to have a testing suite which would properly evaluate their accuracy.

We propose a testing suite that contains a minimum number of test scenes which maximally exercise these implementations and an equivalent number of the reference images that we consider to be the ground truth, to our best knowledge. These are encapsulated in an automated workflow, which runs the tests with a single command and shows the results in form of a website. The suite also contains the data such as code snippets that can be easily integrated into any standard renderer.

The benchmark follows a few basic principles:

\begin{description}
	\item[Easy to use] The benchmark should provide a user-friendly environment that is comprehensible for an average developer or tester of the rendering features. Therefore, the whole suite is written in Python3 as it is currently one of the most popular scripting languages, it does not need to be compiled and is cross-platform. It also provides CLI command options that are invoke-able via python command.
	\item[Modularity] Each part of the benchmark should be adjustable without the need to heavily modify the other parts. For example, if a new CLI option is to be added, you only need to change the \texttt{/src/arg\_parser.py} file.
	\item[Extensibility] It should be simple enough to extend the capabilities of the benchmark, such as adding new scenes, test case scenarios or even renderers. For example, you don't have to modify any code if you want to add a new scene --- there are structures prepared for this scenario which simply need to be filled.
	\item[Simplicity] The scenes are straightforward, containing only basic and portable geometry, light sources and cameras. This brings two large advantages --- it is fairly easy to replicate them for different renderers and they are simple enough to understand the purpose of each element they contain. Along with the thorough comments, anyone with the basic knowledge acquired in the previous chapters of this thesis should comprehend their meaning.
	\item[Standalone] The benchmark should contain all the data that the potential user would need to properly run or generally use the testing suite. For example, the geometry that is included in the scenes can be found in the \texttt{/data/common/} folder.
\end{description}

\section{Framework}

First of all, we take a look into the framework of the benchmark suite and its structure. The file organization is demonstrated in \autoref{fig:framework} and the following sections describe each major subsection of it.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/framework.pdf}
	\caption{File organization of the benchmark}
	\label{fig:framework}
\end{figure}

\subsection{Code}

The Python code here simplifies the benchmark process and ensures that the user is working with the benchmark correctly. It has only basic functionalities that generally process the inputs and run the renderer accordingly. We provide a quick overview of each of the Python files:

\begin{description}
	\item[src/arg\_parser.py] Parses the CLI arguments and the \texttt{settings.json} file and fills its variables accordingly.
	\item[src/constants.py] Simply contains the constants that are used in other scripts.
	\item[src/normalizer.py] This script is called after the benchmark is done to normalize the resulting images, as each renderer might have unique naming conventions. It is used only in corner cases.
	\item[src/visualizer.py] Runs a HTTP server which is required by jeri.io to upload EXR images and opens the website with the results.
	\item[benchmark.py] The first script that is intended to be directly invoked by the user. Runs other helper scripts mentioned above, his purpose is to actually call the rendering executable for each of the scenes found in \texttt{/data/cases/} accordingly to their \texttt{configuration.json}.
	\item[visualize.py] The second script that is intended to be directly invoked by the user. It serves as a wrapper around \texttt{src/visualizer.py}.
\end{description}

The choice for Python3 is therefore obvious --- for these purposes, there is no need for any high performance or structural design solutions. Thanks to that, we don't have to force the user to compile anything and the benchmark is immediately after the download ready to use.

\subsection{Cases}

The folder \texttt{/data/cases/} contains the actual scene descriptions along with their configurations. They follow the structure:

\begin{lstlisting}
/data/cases/<case name>/<renderer>/scenes+configuration
\end{lstlisting}

The benchmark script browses this sctructure, looking for the \texttt{configuration.json} file. The purpose of these configurations is that for different scenes we might want to provide different renderer parameters, such as spectral or polarized modes or various definitions.

Each scene is explained in great detail in \autoref{sec:scenes}.

\subsection{Common data}

The folder \texttt{/data/common} contains the information and values that are used in the renderers. The built-in definitions of the geometry or the illumination might be unique for each renderer so it is convenient to have such information in a unified form. The folder contains:

\begin{description}
	\item[macbeth\_colors/] Spectral values for all Macbeth colors (24 patch version\footnote{\url{https://xritephoto.com/ph_product_overview.aspx/?id=1192&catid=28}} as defined in ART
	\item[CIE\_D50\_illuminant] Spectral values for CIE D50 illuminant~\cite{cieData} rescaled to be used in Mitsuba2
	\item[envmap.exr] Museum environment map by Bernhard Vogl\footnote{\url{http://dativ.at/lightprobes/}}
	\item[sphere.obj/rectangle.obj] Unit sphere object and square object with the length of the side equal to 2
\end{description}

\subsection{Code snippets}

Some of the evaluated computations at least partially contribute to the material BSDF, hence it is possible to express them in a generalized form that is easily integrable into any conventional renderer. We decided to provide the code snippets written in C++ (stored in the folder \texttt{/data/snippets/}) so that any future user might utilize these to implement them into his own renderer. The folder contains:

\begin{description}
	\item[iridescence\_term.cpp] Computation of the iridescence term along with the helper functions, inspired by the code created by \citet{belcour2017practical}
	\item[reflectance\_ggx.cpp] Contains the methods for sampling, evaluating and masking according to the GGX reflectance definition~\cite{walter2007microfacet}
	\item[types.h] Structures used in the snippets mentioned above 
\end{description}

\subsection{JERI}

For the user's convenience, we decided to integrate an EXR visualizer. As it is an extra addition, we decided to use an existing one instead of creating our own.

\emph{JERI} (or Javascript Extended-Range Image) is an EXR viewer written in JavaScript developed by Disney Research~\cite{jeriWeb}. It is simple to use and to integrate and provides many features over the images such as zoom, change of exposure and automatic error maps.

We use JERI to display the results of the whole benchmark to the user on a single website. The user may look at the results, the reference images and even at their differences (compared by L2 and MAPE error maps). A screenshot of the results website is shown in \autoref{fig:screenshot}.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/screenshot.png}
	\caption{Results website}
	\label{fig:screenshot}
\end{figure}

\section{Supported Spectral Renderers}

\subsection{Mitsuba2}

\subsection{ART}

\section{Scenes}
\label{sec:scenes}

\subsection{GGX Reflectance}

\subsection{Spectral accuracy}

\subsection{Polarization}

\subsection{Fluorescence}

\subsection{Iridescence}

\section{Usage}

The user may clone/download the benchmark from the github repository of this thesis\footnote{\url{https://github.com/marcel1hruska/render_benchmark}}. As the whole benchmark is written in Python3, there is no need to compile the project. Then, it is necessary to modify \texttt{settings\.json} file in the root directory of the benchmark suite --- the user must specify the path to the renderer executable and the name of the renderer (currently, only \texttt{mitsuba2}) and \texttt{ART} are viable options).

That is all for the preparations and the benchmark can be run with simply running the \texttt{benchmark.py} python file in the CLI. Be aware that the script must be run from within the root directory of the benchmark suite! The following commands will get you to the root directory and run the benchmark.

\begin{lstlisting}
cd /render_benchmark
python3 ./benchmark.py
\end{lstlisting}

As soon as the benchmark ends, the user may find the rendered EXR images in a folder named \texttt{outputs-yyymmdd-hhmmss} where the \texttt{yyymmdd-hhmmss} is substituted for the date and time when the command was issued.

Then, a user may use a second script \texttt{visualize.py} that opens the website with the results of the benchmark, the reference images and their difference images. This script must also be run from without the root directory of the benchmark suite.

The benchmark accepts several parameters:
\begin{description}
	\item[-{}-help (-h)] Only outputs the help message
	\item[-{}-scene (-s) \textless scene\_name\textgreater] Test only the scene called scene\_name
	\item[-{}-case (-c) \textless test\_case\_name\textgreater] Test only the test case called test\_case\_name
	\item[-{}-renderer (-r) [ART/mitsuba2]] Specify the name of the renderer 
	\item[-{}-exec (-e) \textless path\textgreater] Specify the path to the renderer executable
	\item[-{}-log (-l)] Write all outputs to the log file (in the outputs folder )instead of the console
	\item[-{}-visualize (-v)] Visualize the outputs immediately after the benchmark ends
\end{description}

For convenience, all of these options may be specified in the \texttt{settings.json} file but the ones defined in the CLI override them.

In the following sections, we demonstrate the basic uses cases of our testing suite using a fictional persona called Frodo.

\subsection{Use case Regress test}

Frodo is a mitsuba2 developer who recently changed the sampling strategies of the spectral rendering. However, he is not sure whether he broke some of the functionalities. He sets the benchmark for mitsuba2 and provides it his latest executable. He runs all test case scenarios and compares the results with the reference images.
 
\subsection{Use case New feature}
\label{sec:frodo}
Frodo is an ART developer who would like to add the GGX microfacet distribution to ART as it is not currently supported in the latest version. He finds the code snippets that are attached to the benchmark suite and integrates them to ART.

Then, he looks up the scenes that test GGX reflectance created for mitsuba2 and, as these contain only a very simple a straightforward geometry, he duplicates them. He saves them in a folder \texttt{data/cases/reflectance/ART/} and creates \texttt{configuration.json} file in the same folder that only specificies the name of the resulting image, the filename of the test scene and the parameters that are to be passed to the rendered (examples can be seen in different ART scenarios).

He then provides the executable to the benchmark suite, sets it to ART and runs it. The benchmark automatically detects the new configuration for ART. Either from the reference images or from the difference images, he may determine some irregularities that are caused by his incorrect implementation of the masking function. As soon as he corrects it, he sees that the results are coherent.

\subsection{Use case New scenario}

Frodo is a mitsuba2 developer and he just added support for dispersion. As the dispersion has never been tested before, he simply adds a new folder to the benchmark suite \texttt{/data/cases/dispersion/mitsuba2} and, as in the previous case, he creates the \texttt{configuration.json} for it.

Now, he must add the keyword \texttt{dispersion} to the string array \texttt{TEXT\_CASES} that can be found in the file \texttt{/src/constants.py}.

From now on, the benchmark contains the dispersion tests as well. If he wants to add the reference images as well, he may simply add them to the \texttt{/data/references} folder.

In case he also wants to add the new scenario to the visualizer, he needs to extend the file \texttt{/jeri/page/results\_viewer.html}. But, as the HTML page contains a simple JSON structure for the elements that it should contain, it is fairly easy to do so.

\subsection{Use case Improved feature}

Frodo is an ART developer who reworked the fluorescent materials and would like to see if the new implementation improved them.

The test scenes may be considered as simple templates with whom we created the reference images. Frodo simply adjusts the test scenes in the fluorescence test case scenario. If he finds out that the results indeed improved, he may simply replace the existing reference images and keep the adjusted scenes.

\subsection{Use case New renderer}

Frodo has created his own spectral renderer the Ring that supports all the test case scenarios in the benchmark suite and he wants to evaluate the correctness of his implementations.

Therefore, he duplicates the template scenes from other renderers accordingly to his own scene format. He creates new folders in each test case scenarios in the \texttt{/data/cases/} folder, puts his scenes inside accordingly and create the \texttt{configuration.json} for each of them.

Then, he adds support for his renderer --- in \texttt{src/constants.py}, he adds the \texttt{ring} keyword to the string array \texttt{RENDERERS}.

Now, he can set the benchmark to evaluate the new ring renderer and provides his executable. The benchmark pipeline as well as the visualization is done for it automatically.

\section{Open source contributions}

During our work on the benchmark, we have done several noteworthy contributions to three open source projects. Note that these extensions can be considered as byproducts and definitely not the main aim of this thesis --- therefore, they are not yet in a form a pull request as this process requires a significant amount of time.

\subsection{GGX for ART}

The use case described in \autoref{sec:frodo} actually happened to us (not to Frodo). As a part of the work on the benchmark, we've decided to add the GGX distribution to the ART renderer and, coincidentally, it nicely correlated with the mentioned scenario. We had the scenes and the reference images prepared for mitsuba2, we simply replicated them for ART and implemented the GGX. Then, we simply iterated the benchmark and the adjustments in the code over and over until the results were satisfactory.

The implementation is attached to the thesis as GGX\_ART.

\subsection{Iridescence for Mitsuba2}

Mitsuba does not have a native support for the iridescence but an external plugin has been developed to simulate the iridescent effects of a thin film dielectric layer on top of a rough conductor base. Unfortunately, it was created for Mitsuba version 0.6 and, as Mitsuba2 is fairly new, there has not been an effort to rework the plugin, thus we took the initiative and re-implemented it.

The whole work is based on the \citet{belcour2017practical} and the supplementary code for Mitsuba 0.6 released along with it.
There were some major changes to the spectral sampling strategy and the overall object structure done in Mitsuba2 that had to be adapted in the new, re-implemented version of the plugin.

The correctness of the rework has been confirmed in a similar manner to the normal benchmark workflow. We prepared the test case scenerio scenes for the iridescence, rendered them for both Mitsuba2 and Mitsuba0.6 and considered the latter version to be the ground truth. The implementation can be found on \url{https://github.com/marcel1hruska/mitsuba2}.

\subsection{Multi-channel EXR support for jeri.io}

While designing the polarization test scenes, we've encountered a compatibility issue between the Stokes vector output format of Mitsuba2 and ART. While ART stores the Stokes vector values into distinct EXR images, Mitsuba2 creates a single multi-channel EXR where each channel contains the Stokes vector information. As you can imagine, the visualization of the rendering results requires a custom solution --- it consists of two parts. We've added a support for multi-channel EXR images to the jeri.io as there is currently no way to visualize them. It works as follows:

\begin{enumerate}
	\item If the EXR image contains multiple channels, store them in a extra map \texttt{otherChannels} which connects channel name with its contents.
	\item The user may specify the channel to display in the viewer data.
	\item If the specified channel is in the map, display the wanted contents
\end{enumerate}

The second part puts one more layer on top of that and highly custom for our purposes of resolving the incompatibility itself. We assume the format of ART --- e.g. the file named \texttt{sphere.s0.exr} means that it is the output of the first channel of the Stokes vector. Our script in jeri.io tests whether this file really exists --- if not, we assume from the name of the file that the user actually wants the channel \texttt{S0} of the file \texttt{sphere.exr} and therefore we switch to that.

This behavior is transparent to the user. If the file really does not exist, the jeri.io fails to find as any other file and displays nothing.

This addition is only a part of the compiled JavaScript code of the jeri.io. It can be found in the benchmark's file \texttt{/jeri/exr-worker.js} and \texttt{/jeri/jeri.js} between the lines commented as \texttt{multichannel custom support}.

\section{Future Work}